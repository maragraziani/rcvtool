{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mara/venv/local/lib/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.engine import Layer\n",
    "import keras.backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Gradient Reversal Layer implementation for Keras\n",
    "Credits:\n",
    "https://github.com/michetonu/gradient_reversal_keras_tf/blob/master/flipGradientTF.py\n",
    "\"\"\"\n",
    "\n",
    "def reverse_gradient(X, hp_lambda):\n",
    "    '''Flips the sign of the incoming gradient during training.'''\n",
    "    try:\n",
    "        reverse_gradient.num_calls += 1\n",
    "    except AttributeError:\n",
    "        reverse_gradient.num_calls = 1\n",
    "\n",
    "    grad_name = \"GradientReversal%d\" % reverse_gradient.num_calls\n",
    "\n",
    "    @tf.RegisterGradient(grad_name)\n",
    "    def _flip_gradients(op, grad):\n",
    "        return [tf.negative(grad) * hp_lambda]\n",
    "\n",
    "    g = K.get_session().graph\n",
    "    with g.gradient_override_map({'Identity': grad_name}):\n",
    "        y = tf.identity(X)\n",
    "    return y\n",
    "\n",
    "class GradientReversal(Layer):\n",
    "    '''Flip the sign of gradient during training.'''\n",
    "    def __init__(self, hp_lambda, **kwargs):\n",
    "        super(GradientReversal, self).__init__(**kwargs)\n",
    "        self.supports_masking = False\n",
    "        self.hp_lambda = hp_lambda\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.trainable_weights = []\n",
    "        return\n",
    "    def call(self, x, mask=None):\n",
    "        return reverse_gradient(x, self.hp_lambda)\n",
    "\n",
    "    def get_output_shape_for(self, input_shape):\n",
    "        return input_shape\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {}\n",
    "        base_config = super(GradientReversal, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CAMLP():\n",
    "    \n",
    "    \"\"\"Concept Adversarial MLP\"\"\"\n",
    "    def feature_extractor(self, inp):\n",
    "            ''' \n",
    "            This function defines the structure of the feature extractor part.\n",
    "            '''\n",
    "            if self.channels<3:\n",
    "                out = keras.layers.Flatten(input_shape=(self.input_shape,self.input_shape))(inp)\n",
    "            else: \n",
    "                out = keras.layers.Flatten(input_shape=(self.input_shape,self.input_shape, self.channels))(inp)\n",
    "            \n",
    "            counter = 0\n",
    "            while counter<self.height:\n",
    "                out = keras.layers.Dense(self.width, activation=keras.layers.Activation('relu'))(out)    \n",
    "                counter +=1\n",
    "            self.domain_invariant_features = out\n",
    "            return out\n",
    "        \n",
    "    def classifier(self, inp):\n",
    "        ''' \n",
    "        This function defines the structure of the classifier part.\n",
    "        '''\n",
    "        #out = kl.Dense(128, activation=\"relu\")(inp)\n",
    "        #out = kl.Dropout(0.5)(out)\n",
    "        out = keras.layers.Dense(self.classes, activation=\"softmax\", name=\"classifier_output\")(inp)\n",
    "        return out\n",
    "    def concept_regressor(self, inp):\n",
    "        ''' \n",
    "        This function defines the structure of the concept regression part.\n",
    "        '''\n",
    "        out = keras.layers.Dense(1, activation=\"linear\")(inp)\n",
    "        return out\n",
    "\n",
    "\n",
    "\n",
    "    def compile(self, optimizer):\n",
    "        '''\n",
    "        This function compiles the model based on the given optimization method and its parameters.\n",
    "        '''\n",
    "        self.model.compile(optimizer=optimizer, \n",
    "                           loss={'classifier_output': 'binary_crossentropy', \n",
    "                                 'discriminator_output': 'binary_crossentropy'}, \n",
    "                           loss_weights={'classifier_output': 0.5, \n",
    "                                         'discriminator_output': 1.0})\n",
    "\n",
    "    def _build(self):\n",
    "            '''\n",
    "            This function builds the network based on the Feature Extractor, Classifier and Discriminator parts.\n",
    "            '''\n",
    "            inp = keras.layers.Input(shape=(self.input_shape,self.input_shape), name=\"main_input\")\n",
    "            feature_output = self.feature_extractor(inp)\n",
    "            import pdb; pdb.set_trace()\n",
    "            ###\n",
    "            self.grl_layer = GradientReversal(1.0)\n",
    "            feature_output_grl = self.grl_layer(feature_output)\n",
    "            labeled_feature_output = keras.layers.Lambda(lambda x: \n",
    "                                                           K.switch(K.learning_phase(), \n",
    "                                                                      K.concatenate([x[:int(self.batch_size//2)], x[:int(self.batch_size//2)]], axis=0), x), \n",
    "                                                            output_shape=lambda x: x[0:])(feature_output_grl)\n",
    "            ###\n",
    "            classifier_output = self.classifier(labeled_feature_output)\n",
    "            discriminator_output = self.concept_regressor(feature_output)\n",
    "            model = keras.models.Model(inputs=inp, outputs=[discriminator_output, classifier_output])\n",
    "            return model\n",
    "        \n",
    "    \n",
    "    def __init__(self, deep=2, wide=512, channels=3, features=1, grl = 'auto', optimizer='SGD', lr=1e-2, epochs=10,\n",
    "                 batch_size=32, input_shape=28, classes=10, summary = False, model_plot=False, **kwargs):\n",
    "        self.learning_phase = K.variable(1)\n",
    "        self.domain_invariant_features = None\n",
    "        self.width, self.height, self.channels = wide, deep, channels\n",
    "        self.input_shape =  input_shape #(channels, width, eight)\n",
    "        self.classes = classes\n",
    "        self.features = features\n",
    "        self.batch_size = batch_size\n",
    "        self.gr1 = 'auto'\n",
    "        # Set reversal gradient value\n",
    "        if grl is 'auto':\n",
    "            self.grl_rate = 1.0\n",
    "        else:\n",
    "            self.grl_rate = grl\n",
    "        self.summary = summary\n",
    "        self.model_plot = model_plot\n",
    "        \n",
    "        # Build the model\n",
    "        self.model = self._build()\n",
    "        # Print and Save the model summary if requested.\n",
    "        if self.summary:\n",
    "            self.model.summary()\n",
    "        if self.model_plot:\n",
    "            plot_model(self.model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mara/venv/local/lib/python2.7/site-packages/keras/activations.py:115: UserWarning: Do not pass a layer instance (such as Activation) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> <ipython-input-4-82c3b6a29e1f>(55)_build()\n",
      "-> self.grl_layer = GradientReversal(1.0)\n",
      "(Pdb) c\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mara/venv/local/lib/python2.7/site-packages/keras/engine/topology.py:638: UserWarning: Class `__main__.GradientReversal` defines `get_output_shape_for` but does not override `compute_output_shape`. If this is a Keras 1 layer, please implement `compute_output_shape` to support Keras 2.\n",
      "  output_shape = self.compute_output_shape(input_shape)\n"
     ]
    }
   ],
   "source": [
    "damlp = CAMLP()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
